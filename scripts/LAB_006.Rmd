---
title: "LAB_006"
author: "Mike James"
date: "February 25, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

***

As for Homework_001:  

* The data was not normal. Unequal variances.  

* They were sampling two very different forests.  

* Appropriate to use the paired t-test at that time (1989?); things change.   

(x/25 points per homework)

More strict (in grading starting with homework_003)

***

# Power question:

> How long does it take to determine a "trend" on a monitoring (logitudinal) project?

This was on the board when I got here.

***

# ISwR

You may not want to run ANOVA on the data, but on the slopes of many different analyses (???)  

* LM = linear model
* AOV = Analysis of Variants (calls to GLM)
* ANOVA = call of sum of squares partitioning protocol  

**AOV** will be the most commonly used in R  

Testing for assumptions, you need the error resiiduals first.  

Residuals will come up during QC. (???)  

> Homoscedastic: In statistics, a sequence or a vector of random variables is homoscedastic if all random variables in the sequence or vector have the same finite variance. This is also known as homogeneity of variance. The complementary notion is called heteroscedasticity. The spellings homoskedasticity and heteroskedasticity are also frequently used.  

***

```{r}
library(ISwR)
attach(red.cell.folate)
summary(red.cell.folate)
boxplot(folate~ventilation)
```

So, what do we see here? 

```{r}
rcf.lm <- lm(folate~ventilation)
anova(rcf.lm)
```

ANOVA is partitioning the linear data into a variance table.

LM is calculating the overall mean, and the mean of each treatment, and then extract the error mean.  

```{r}
xbar <- tapply(folate, ventilation, mean)
s <- tapply(folate, ventilation, sd)
n <- tapply(folate, ventilation, length)
sem <- s/sqrt(n)
stripchart(folate~ventilation, method="jitter", jit=0.05, pch=20, vert=T)

# pch = Point Character (type of points in the graph)

arrows(1:3, xbar+sem, 1:3, xbar-sem, angle=90, code=3, length=0.1)
lines(1:3, xbar, pch=4, type="b", cex=2)
```

The line shows there is some *linkage* between the points (in this case, this is **bad** (and in most cases it is bad, it seems)).

```{r}
xval <-barplot(xbar, ylim=c(0,350))
arrows(xval, xbar+sem, xval, xbar-sem, angle=90, code=3, length=0.1)
```

Now we're going to "test out assumptions" from the residuals.  

```{r}
qqnorm(rcf.lm$residuals)
opar <- par(mfrow=c(2,2), mex = 0.7, mar = c(4,4,3,2)+0.3) # open parameters
plot(rcf.lm, which = 1:4)
par(opar) # closes the parameters

# mfrow = matrix formatted as rows
# mex = space between graphs
# mar = matrix arrangement

# These all show that the data is behaving normally
```

```{r}
shapiro.test(rcf.lm$residuals)
```

```{r}
# This is a modified F test

bartlett.test(folate~ventilation)
```

We have been unable to reject $H_{0}$

Can't say "Jack shit" about our data; we're just number crunchers. We have no idea how the data was collected, etc.  

# Post-Hoc / Pairwise t-tests

```{r}
pairwise.t.test(folate, ventilation, p.adjust.method = "bonferroni", pooled.sd=T)
```

```{r}
# Tukey's HSD
rcf.aov <- aov(folate~ventilation)
anova(rcf.aov)
TukeyHSD(rcf.aov)
```

If effects are VERY small, always use Tukey's HSD.  

Tukey's is very powerful.

***

# Moving on to Power Analysis

```{r}
pad <- read.csv("C:/Users/Mike/Dropbox/SLU/BIOMETRY/LAB/Data/power.analysis.data.csv")
View(pad)
```

Normality is the mean of its **own** data, not the whole dataset

```{r}
attach(pad)
boxplot(weight~treatment)
```

```{r}
summary(pad)
```


```{r}
trt.means<-tapply(weight, treatment, mean)
trt.means
trt.sd <- tapply(weight, treatment, sd)
trt.sd
trt.sample.size<-tapply(weight, treatment, length)
trt.sample.size
trt.sem<-(trt.sd/sqrt(trt.sample.size))
trt.sem
```
Perform anova and save

```{r}
test1.aov <-aov(weight~treatment)
```

To asses the normality, I can asses a shapiro and barlett test
```{r}
bartlett.test(weight~treatment)
shapiro.test(test1.aov$residuals)
plot(test1.aov)
summary(test1.aov)
```
So test lets us know it is normal and the variances are not significantly different.

Import POWER ANALYSIS
```{r}
library(pwr)
```

k is the number of contrast you are perfroming.
In any mathematical equation is

```{r}
pwr.anova.test(k=4, f=0.08,power=0.70,sig.level=0.05)
```

This shows you will need 345 rats per treatment.

```{r}
pwr.anova.test(k=4, f=0.08,power=0.80,sig.level=0.05)
```

This shows the need for 427 rats

```{r}
pwr.anova.test(k=4, f=0.08,power=0.70,sig.level=0.1)
```

270 rats

```{r}
pwr.anova.test(k=3, f=0.08,power=0.70,sig.level=0.1)
```

312 rats

The number of rats could be indicative that the effect is not there.

```{r}
pwr.anova.test(k=4, n=12, power=0.80, sig.level=0.05)
```

* 4 contrasts
* 12 rats
* effect 50% increase between largest and smallest

```{r}
pwr.anova.test(k=4, n=18, power=0.80, sig.level=0.05)
```

40%

```{r}
pwr.anova.test(k=4, n=18, power=0.70, sig.level=0.05)
```

36%  

Power is the ability of the test to give you the right answer.  

> Contrasts are the number of experiments  

